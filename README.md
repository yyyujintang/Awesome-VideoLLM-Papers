# Awesome-VideoLLM-Papers
This repository compiles a list of papers related to Video LLM.  Continual improvements are being made to this repository. If you come across any relevant papers that should be included, please don't hesitate to open an issue.

## Survey

(Arxiv23.06.23) A Survey on Multimodal Large Language Models [Paper](https://arxiv.org/abs/2306.13549) [Code](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) 

(Arxiv23.11.10) How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model [Paper](https://arxiv.org/abs/2311.07594)

(Arxiv23.11.21) A Survey on Multimodal Large Language Models for Autonomous Driving [Paper](https://arxiv.org/abs/2311.12320)

(Arxiv24.01.10) Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning [Paper](https://arxiv.org/abs/2401.06805) 

(Arxiv24.02.01) Safety of Multimodal Large Language Models on Images and Text [Paper](https://arxiv.org/abs/2402.00357) [Code](https://github.com/isXinLiu/MLLM-Safety-Collection) 

(Arxiv24.02.19) The (R)Evolution of Multimodal Large Language Models: A Survey [Paper](https://arxiv.org/abs/2402.12451) 

(Arxiv24.05.17) Efficient Multimodal Large Language Models: A Survey [Paper](https://arxiv.org/abs/2405.10739) [Code](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey) 

## Video Understanding

(Arxiv23.09.01) HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving [Paper](https://arxiv.org/abs/2309.05186) 

(Arxiv23.10.02) DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model [Paper](https://arxiv.org/abs/2310.01412) 

(Arxiv23.11.25) GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation [Paper](https://arxiv.org/abs/2311.16511) 

(Arxiv23.11.28) MVBench: A Comprehensive Multi-modal Video Understanding Benchmark [Paper](https://arxiv.org/abs/2311.17005) 

(Arxiv23.12.05) EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model [Paper](https://arxiv.org/abs/2312.02483) 

(Arxiv24.01.02) Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models [Paper](https://arxiv.org/abs/2401.00988) 

(Arxiv24.01.18) Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models [Paper](https://arxiv.org/abs/2309.05186) 

(Arxiv24.01.26) From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities [Paper](https://arxiv.org/abs/2401.15071) 

(Arxiv24.03.25) Elysium: Exploring Object-level Perception in Videos via MLLM [Paper](https://arxiv.org/abs/2403.16558) 

(Arxiv24.04.18) From Image to Video, what do we need in multimodal LLMs? [Paper](https://arxiv.org/abs/2404.11865) 

(Arxiv24.04.25) Semantically consistent Video-to-Audio Generation using Multimodal Language Large Model [Paper](https://arxiv.org/abs/2404.16305) 

(Arxiv24.04.28) WorldGPT: Empowering LLM as Multimodal World Model [Paper](https://arxiv.org/abs/2404.18202) 

(Arxiv24.05.01) EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos with Multi-modal Large Language Model [Paper](https://arxiv.org/abs/2405.00574) 

(Arxiv24.05.13) FreeVA: Offline MLLM as Training-Free Video Assistant [Paper](https://arxiv.org/abs/2405.07798) 

(Arxiv24.05.31) Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis [Paper](https://arxiv.org/abs/2405.21075) 

## Other Useful Sources

[Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)

[VLM-Eval: A General Evaluation on Video Large Language Models](https://github.com/zyayoung/Awesome-Video-LLMs)

[LLMs Meet Multimodal Generation and Editing: A Survey](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)

